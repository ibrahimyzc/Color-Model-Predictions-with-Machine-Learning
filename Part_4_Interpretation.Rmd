---
title: "Part 4 - Interpretation"
author: "Ibrahim Yazici"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load packages

This example uses the `tidyverse` suite of packages.  

```{r, load_tidyverse}
library(tidyverse)
```

## Read data

The code chunk below reads in the final project data.  

```{r, read_final_data}
df <- readr::read_csv("paint_project_train_data.csv", col_names = TRUE)
```

The `readr::read_csv()` function displays the data types and column names associated with the data. However, a glimpse is shown below that reveals the number of rows and also shows some of the representative values for the columns.  

```{r, show_data_glimpse, eval = FALSE}
df %>% glimpse()
```

The data consist of continuous and categorical inputs. The `glimpse()` shown above reveals the data type for each variable which state to you whether the input is continuous or categorical. The RGB color model inputs, `R`, `G`, and `B` are continuous (dbl) inputs. The HSL color model inputs consist of 2 categorical inputs, `Lightness` and `Saturation`, and a continuous input, `Hue`. Two outputs are provided. The continuous output, `response`, and the Binary output, `outcome`. However, the data type of the Binary outcome is numeric because the Binary `outcome` is **encoded** as `outcome = 1` for the EVENT and `outcome = 0` for the NON-EVENT.  

The code chunk below assembles the data for interpretation of the best regression model. The logit-transformed output is named `y`. The `dfii` dataframe as the original `response` and Binary output, `outcome`, removed. This way we can focus on the variables specific to the regression task.  


```{r, make_reg_data}
dfii <- df %>% 
  mutate(y = boot::logit( (response - 0) / (100 - 0) ) ) %>% 
  select(R, G, B, 
         Lightness, Saturation, Hue,
         y)
```

The code chunk below assembles the data for interpretation of the best classification model.

```{r, make_iiiD_data}
dfiiiD <- df %>% 
  select(-response) %>% 
  mutate(outcome = ifelse(outcome == 1, 'event', 'non_event'),
         outcome = factor(outcome, levels = c('event', 'non_event')))
```

By converting `outcome` to a factor, the unique values of the variables are "always known":  

```{r, show_outcome_levels}
dfiiiD %>% pull(outcome) %>% levels()
```

However, the value counts are the same as the original encoding.  

```{r, confirm_outcome_Counts}
dfiiiD %>% count(outcome)
```


## Interpretation of the Results

### Input Importance

#### i) Regression

In `Part_2_Regression` file, by "RMSE" and "Rsquared" metric values, we identified the best model as: "Add Categorical Inputs to Interactions from 3 DOF spline from input R and All Pairwise Interactions of Continuous Inputs G, B, Hue (This is Model 9 in Part iiA)".

Lets call this model as `best_reg_model` and load it below: 

```{r, a_1}
library(caret)
```

```{r, a_2}
best_reg_model <- readr::read_rds("best_regression_model.rds")
```


We can see the most important variables associated with our best performing regression model, `best_reg_model`, below.

```{r, a_3_1, eval = FALSE}
var_imp_reg_best <- varImp(best_reg_model, scale = TRUE)
print(var_imp_reg_best)
```

We can see the full list of 39 variables below, to identify the least important ones as well.

```{r, a_3_11, eval = FALSE}
top_n_reg_best <- head(var_imp_reg_best, n = 39)
print(top_n_reg_best)
```

We can plot the importanceâ€™s below.

```{r, a_3_111, eval = FALSE}
plot(var_imp_reg_best)
```

#### ii) Classification

In `Part_3_Classification` file, by "Accuracy" metric values, we identified the best model as: Gradient boosted tree.

Lets call this model as `best_class_model` and load it below:

```{r, a_2_0}
best_class_model <- readr::read_rds("best_classification_model.rds")
```


We can see the most important variables associated with our best performing classification model, `best_class_model`, below.

```{r, a_6, eval = FALSE}
var_imp_class_best <- varImp(best_class_model, scale = TRUE)
print(var_imp_class_best)
plot(var_imp_class_best)
```

Below we make a surface plot for the hardest to predict Lightness and Saturation combinations in regression:

```{r, c_1, eval = FALSE}
primary_seq <- seq(min(dfii$R), max(dfii$R), length.out = 101)
secondary_seq <- seq(min(dfii$B), max(dfii$B), length.out = 101)

prediction_data <- expand.grid(
  R = primary_seq,
  B = secondary_seq,  
  Hue = mean(dfii$Hue), 
  G = mean(dfii$G),
  Lightness = "saturated",
  Saturation = "muted" 
)

prediction_data$predictions <- predict(best_reg_model, newdata = prediction_data)

ggplot(prediction_data, aes(x = R, y = B, fill = predictions)) +
  geom_raster(interpolate = TRUE) +
  scale_fill_gradientn(colors = terrain.colors(10)) +
  theme_minimal() +
  labs(x = "R", y = "B", fill = "Predicted Value")
```

Below we make a surface plot for the easiest to predict Lightness and Saturation combinations in regression:

```{r, c_11, eval = FALSE}
primary_seq <- seq(min(dfii$R), max(dfii$R), length.out = 101)
secondary_seq <- seq(min(dfii$B), max(dfii$B), length.out = 101)

prediction_data <- expand.grid(
  R = primary_seq,
  B = secondary_seq,  
  Hue = mean(dfii$Hue), 
  G = mean(dfii$G),
  Lightness = "deep",
  Saturation = "neutral" 
)

prediction_data$predictions <- predict(best_reg_model, newdata = prediction_data)

ggplot(prediction_data, aes(x = R, y = B, fill = predictions)) +
  geom_raster(interpolate = TRUE) +
  scale_fill_gradientn(colors = terrain.colors(10)) +
  theme_minimal() +
  labs(x = "R", y = "B", fill = "Predicted Value")
```


Below we make a surface plot for the hardest to predict Lightness and Saturation combinations in classification:

```{r, c_2, eval = FALSE}
primary_seq <- seq(min(dfiiiD$Hue), max(dfiiiD$Hue), length.out = 101)
secondary_seq <- seq(min(dfiiiD$G), max(dfiiiD$G), length.out = 101)

prediction_data <- expand.grid(
  Hue = primary_seq,
  G = secondary_seq,  
  B = mean(dfiiiD$B), 
  R = mean(dfiiiD$R),
  Lightness = "saturated",
  Saturation = "pure" 
)

pred_probs <- predict(best_class_model, newdata = prediction_data, type = "prob")

prediction_data_df <- prediction_data %>% bind_cols(pred_probs)

ggplot(prediction_data_df, aes(x = Hue, y = G, fill = event)) +
  geom_raster(interpolate = TRUE) +
  scale_fill_gradient2(low = 'blue', mid = 'white', high = 'red', midpoint = 0.5, limits = c(0, 1)) +
  labs(fill = "Event Probability") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

Below we make a surface plot for the easiest to predict Lightness and Saturation combinations in classification:

```{r, c_22, eval = FALSE}
primary_seq <- seq(min(dfiiiD$Hue), max(dfiiiD$Hue), length.out = 101)
secondary_seq <- seq(min(dfiiiD$G), max(dfiiiD$G), length.out = 101)

prediction_data <- expand.grid(
  Hue = primary_seq,
  G = secondary_seq,  
  B = mean(dfiiiD$B), 
  R = mean(dfiiiD$R),
  Lightness = "pale",
  Saturation = "gray" 
)

pred_probs <- predict(best_class_model, newdata = prediction_data, type = "prob")

prediction_data_df <- prediction_data %>% bind_cols(pred_probs)

ggplot(prediction_data_df, aes(x = Hue, y = G, fill = event)) +
  geom_raster(interpolate = TRUE) +
  scale_fill_gradient2(low = 'blue', mid = 'white', high = 'red', midpoint = 0.5, limits = c(0, 1)) +
  labs(fill = "Event Probability") +
  theme_minimal() +
  theme(legend.position = "bottom")
```



