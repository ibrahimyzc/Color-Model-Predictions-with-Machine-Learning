---
title: "Part 3 - Classification"
author: "Ibrahim Yazici"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load packages

We use the `tidyverse` suite of packages.  

```{r, load_tidyverse}
library(tidyverse)
```

## Read data

The code chunk below reads in the final project data.  

```{r, read_final_data}
df <- readr::read_csv("paint_project_train_data.csv", col_names = TRUE)
```

The `readr::read_csv()` function displays the data types and column names associated with the data. However, a glimpse is shown below that reveals the number of rows and also shows some of the representative values for the columns.  

```{r, show_data_glimpse, eval = FALSE}
df %>% glimpse()
```

The data consist of continuous and categorical inputs. The `glimpse()` shown above reveals the data type for each variable which state to you whether the input is continuous or categorical. The RGB color model inputs, `R`, `G`, and `B` are continuous (dbl) inputs. The HSL color model inputs consist of 2 categorical inputs, `Lightness` and `Saturation`, and a continuous input, `Hue`. Two outputs are provided. The continuous output, `response`, and the Binary output, `outcome`. However, the data type of the Binary outcome is numeric because the Binary `outcome` is **encoded** as `outcome = 1` for the EVENT and `outcome = 0` for the NON-EVENT.  

## Binary classification task

The Binary output variable, `outcome`, is a numeric variable.  

```{r, show_outcome_class}
df %>% pull(outcome) %>% class()
```

However, there are **only** two unique values for `outcome`.  

```{r, show_outcome_values}
df %>% count(outcome)
```

As stated previously, `outcome = 1` denotes the **EVENT** while `outcome = 0` denotes the **NON-EVENT**. Thus, the `outcome` variable uses the 0/1 encoding! This encoding is appropriate for `glm()` and the functions we create in homework assignments, and lecture examples. However, `caret` and `tidymodels` prefer a different encoding. For those reasons, two different binary classification data sets are defined. The first will be used for Parts iiiA) and iiiB) while the second will be used for iiiD).  

The data set associated with iiiA) and iiiB) is created below. It removes the `response` variable so that way we can focus on the inputs and binary outcome.  

```{r, make_iiiA_data}
dfiiiA <- df %>% 
  select(-response)
```

The data set associated with iiiD) changes the data type of the `outcome` variable. The `ifelse()` function is used to convert `outcome` to a character data type. The value of `outcome = 1` is converted to the string `'event'` and the value of `outcome = 0` is converted to `'non_event'`. The `outcome` data type is then converted to a factor (R's categorical variable data type) with `'event'` forced as the first level.  

```{r, make_iiiD_data}
dfiiiD <- df %>% 
  select(-response) %>% 
  mutate(outcome = ifelse(outcome == 1, 'event', 'non_event'),
         outcome = factor(outcome, levels = c('event', 'non_event')))
```

By converting `outcome` to a factor, the unique values of the variables are "always known":  

```{r, show_outcome_levels}
dfiiiD %>% pull(outcome) %>% levels()
```

However, the value counts are the same as the original encoding.  

```{r, confirm_outcome_Counts}
dfiiiD %>% count(outcome)
```

We standardize the continuous inputs `R`, `G`, `B`, and `Hue` below.

```{r, standardized}
ready_dfiiiA <- dfiiiA
ready_dfiiiA$R <- scale(ready_dfiiiA$R, center = TRUE, scale = TRUE)
ready_dfiiiA$R <- as.vector(ready_dfiiiA$R) 
ready_dfiiiA$G <- scale(ready_dfiiiA$G, center = TRUE, scale = TRUE)
ready_dfiiiA$G <- as.vector(ready_dfiiiA$G) 
ready_dfiiiA$B <- scale(ready_dfiiiA$B, center = TRUE, scale = TRUE)
ready_dfiiiA$B <- as.vector(ready_dfiiiA$B) 
ready_dfiiiA$Hue <- scale(ready_dfiiiA$Hue, center = TRUE, scale = TRUE)
ready_dfiiiA$Hue <- as.vector(ready_dfiiiA$Hue) 
```

We will use the standardized dataset `ready_dfiiiA` in our classification models in parts A) and B) below.

### A) Linear Models

1) Model1: Intercept Only Model (No Inputs):

```{r, 2A_1}
mod_3A_1 <- glm(outcome ~ 1, data = ready_dfiiiA, family = "binomial")
```

2) Model2: Categorical Variables Only (Linear Additive):

```{r, 2A_2}
mod_3A_2 <- glm(outcome ~ Lightness + Saturation, data = ready_dfiiiA, family = "binomial")
```

3) Model3: Continuous Variables Only (Linear Additive):

```{r, 2A_3}
mod_3A_3 <- glm(outcome ~ R + G + B + Hue, data = ready_dfiiiA, family = "binomial")
```

4) Model4: All Categorical and Continuous Variables (Linear Additive):

```{r, 2A_4}
mod_3A_4 <- glm(outcome ~ R + G + B + Hue + Lightness + Saturation, data = ready_dfiiiA, family = "binomial")
```

5) Model5: Interaction of Categorical Inputs with All Continuous Inputs (Main Effects):

```{r, 2A_5}
mod_3A_5 <- glm(outcome ~ (R + G + B + Hue) * (Lightness + Saturation), data = ready_dfiiiA, family = "binomial")
```

6) Model6: Add Categorical Inputs to All Main Effect and All Pairwise Interactions of Continuous Inputs:

```{r, 2A_6}
mod_3A_6 <- glm(outcome ~ (R + G + B + Hue)^2 + Lightness + Saturation, data = ready_dfiiiA, family = "binomial")
```

7) Model7: Interaction of Categorical Inputs with All Main Effects and All Pairwise Interactions of Continuous Inputs:

```{r, 2A_7}
mod_3A_7 <- glm(outcome ~ (R + G + B + Hue)^2 * (Lightness + Saturation), data = ready_dfiiiA, family = "binomial")
```

8) Model8: Add Categorical Inputs to 3 degree-of-freedom natural (DOF) spline from continuous variables:

```{r, 2A_8}
mod_3A_8 <- glm(outcome ~ splines::ns(R, 3) + splines::ns(G, 3) + splines::ns(B, 3) + splines::ns(Hue, 3) + Lightness + Saturation, data=ready_dfiiiA, family = "binomial")
```

9) Model9: Add Categorical Inputs to Interactions from 3 DOF spline from input R and All Pairwise Interactions of Continuous Inputs G, B, Hue:

```{r, 2A_9}
mod_3A_9 <- glm(outcome ~ splines::ns(R, 3) * (G + B + Hue)^2 + Lightness + Saturation, data=ready_dfiiiA, family = "binomial")
```

10) Model10: Interact Categorical Inputs to Interactions from 3 DOF spline from input R Interactions of Continuous Inputs G, B, Hue:

```{r, 2A_10}
mod_3A_10 <- glm(outcome ~ splines::ns(R, 3) * (G + B + Hue), data=ready_dfiiiA, family = "binomial")
```

Here we note that the following warnings occured when we fit Model 7: "## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred"

The following code chunk calculates AIC values of the models.

```{r, 2A_AIC}
aic_model1 <- AIC(mod_3A_1)
aic_model2 <- AIC(mod_3A_2)
aic_model3 <- AIC(mod_3A_3)
aic_model4 <- AIC(mod_3A_4)
aic_model5 <- AIC(mod_3A_5)
aic_model6 <- AIC(mod_3A_6)
aic_model7 <- AIC(mod_3A_7)
aic_model8 <- AIC(mod_3A_8)
aic_model9 <- AIC(mod_3A_9)
aic_model10 <- AIC(mod_3A_10)

aic_values <- data.frame(
  Model = c("Model 01", "Model 02", "Model 03", "Model 04", "Model 05", "Model 06", "Model 07", "Model 08", "Model 09", "Model 10"),
  AIC = c(aic_model1, aic_model2, aic_model3, aic_model4, aic_model5, aic_model6, aic_model7, aic_model8, aic_model9, aic_model10)
)

ggplot(aic_values, aes(x = Model, y = AIC, fill = Model)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = "AIC Values of Different Models", x = "Model", y = "AIC Value") 
```

The following code chunk calculates BIC values of the models.

```{r, 2A_BIC}
bic_model1 <- BIC(mod_3A_1)
bic_model2 <- BIC(mod_3A_2)
bic_model3 <- BIC(mod_3A_3)
bic_model4 <- BIC(mod_3A_4)
bic_model5 <- BIC(mod_3A_5)
bic_model6 <- BIC(mod_3A_6)
bic_model7 <- BIC(mod_3A_7)
bic_model8 <- BIC(mod_3A_8)
bic_model9 <- BIC(mod_3A_9)
bic_model10 <- BIC(mod_3A_10)

bic_values <- data.frame(
  Model = c("Model 01", "Model 02", "Model 03", "Model 04", "Model 05", "Model 06", "Model 07", "Model 08", "Model 09", "Model 10"),
  BIC = c(bic_model1, bic_model2, bic_model3, bic_model4, bic_model5, bic_model6, bic_model7, bic_model8, bic_model9, bic_model10)
)

ggplot(bic_values, aes(x = Model, y = BIC, fill = Model)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = "BIC Values of Different Models", x = "Model", y = "BIC Value")
```

We will use BIC metric to determine the best model. We will not choose `Model 2` in out list because its AIC metric performance does not look good enough. 

Here are top 3 models:  
1) `Model 8`  
2) `Model 6`  
3) `Model 4`  

Here is the Coefficient summary visualization for `Model 8`.

```{r, 2A_coeff5, eval = FALSE}
coefplot::coefplot(mod_3A_8) +
theme_bw()
```

We can list significant inputs for `Model 8` below.

```{r, 2A_coeff6}
tidy_mod8 <- broom::tidy(mod_3A_8, eval = FALSE)
significant_inputs_mod8 <- tidy_mod8 %>% filter(p.value <0.05)
significant_inputs_mod8
```

Here is the Coefficient summary visualization for `Model 6`.

```{r, 2A_coeff7, eval = FALSE}
coefplot::coefplot(mod_3A_6) +
theme_bw()
```

We can list significant inputs for `Model 6` below.

```{r, 2A_coeff8, eval = FALSE}
tidy_mod6 <- broom::tidy(mod_3A_6)
significant_inputs_mod6 <- tidy_mod6 %>% filter(p.value <0.05)
significant_inputs_mod6
```

Here is the Coefficient summary visualization for `Model 4`.

```{r, 2A_coeff9, eval = FALSE}
coefplot::coefplot(mod_3A_4) +
theme_bw()
```

We can list significant inputs for `Model 4` below.

```{r, 2A_coeff10, eval = FALSE}
tidy_mod4 <- broom::tidy(mod_3A_4)
significant_inputs_mod4 <- tidy_mod4 %>% filter(p.value <0.05)
significant_inputs_mod4
```

### B) Bayesian Linear Models

I will fit the best model (Model 8) and second best model (Model 6) we fit in part A). The reason I pick Model 6 is that I want to compare the two models here again and compare the results with part A).

Here is the design matrix and required information for `Model 8` in Bayesian case.

```{r, 2B_1}
Xmat_8 <- model.matrix(~ splines::ns(R, 3) + splines::ns(G, 3) + splines::ns(B, 3) + splines::ns(Hue, 3) + Lightness + Saturation, data = ready_dfiiiA)

info_8 <- list(
  yobs = ready_dfiiiA$outcome,
  design_matrix = Xmat_8,
  mu_beta = 0,
  tau_beta = 4.5
)
```

Here is the design matrix and required information for `Model 6` in Bayesian case.

```{r, 2B_2}
Xmat_6 <- model.matrix(~ (R + G + B + Hue)^2 + Lightness + Saturation, data = ready_dfiiiA)

info_6 <- list(
  yobs = ready_dfiiiA$outcome,
  design_matrix = Xmat_6,
  mu_beta = 0,
  tau_beta = 4.5
)
```

We define the log-posterior function by completing the code chunk below. 

```{r, 2B_3}
logistic_logpost <- function(unknowns, my_info) {
  # extract the design matrix and assign to X
  X <- my_info$design_matrix
  
  # calculate the linear predictor
  eta <- X %*% unknowns
  
  # calculate the event probability
  mu <- boot::inv.logit(eta)
  
  # evaluate the log-likelihood
  log_lik <- sum(dbinom(x = my_info$yobs, size = 1, prob = mu, log = TRUE))
  
  # evaluate the log-prior
  log_prior <- sum(dnorm(unknowns, mean = my_info$mu_beta, sd = my_info$tau_beta, log = TRUE))
  
  # sum together
  log_lik + log_prior
}
```

We define the `my_laplace()` function is defined for you in the code chunk below. 

```{r, 2B_4}
my_laplace <- function(start_guess, logpost_func, ...)
{
  # code adapted from the `LearnBayes`` function `laplace()`
  fit <- optim(start_guess,
               logpost_func,
               gr = NULL,
               ...,
               method = "BFGS",
               hessian = TRUE,
               control = list(fnscale = -1, maxit = 5001))
  
  mode <- fit$par
  post_var_matrix <- -solve(fit$hessian)
  p <- length(mode)
  int <- p/2 * log(2 * pi) + 0.5 * log(det(post_var_matrix)) + logpost_func(mode, ...)
  # package all of the results into a list
  list(mode = mode,
       var_matrix = post_var_matrix,
       log_evidence = int,
       converge = ifelse(fit$convergence == 0,
                         "YES", 
                         "NO"),
       iter_counts = as.numeric(fit$counts[1]))
}
```

We execute the Laplace Approximation for the Model 8 formulation and the Model 6 formulation. 

```{r, 2B_5}
laplace_8 <- my_laplace(rep(0, ncol(Xmat_8)), logistic_logpost, info_8)
laplace_8$converge
```

```{r, 2B_6}
laplace_6 <- my_laplace(rep(0, ncol(Xmat_6)), logistic_logpost, info_6)
laplace_6$converge
```

We use the Bayes Factor to compare the models. We can conclude that `Model 8` is the better of the models.

```{r, 2B_10}
exp(laplace_8$log_evidence)/exp(laplace_6$log_evidence)
```

A function is defined in the code chunk below. This function creates a coefficient summary plot in the style of the `coefplot()` function, but uses the Bayesian results from the Laplace Approximation.

```{r, 2B_7}
viz_post_coefs <- function(post_means, post_sds, xnames)
{
  tibble::tibble(
    mu = post_means,
    sd = post_sds,
    x = xnames
  ) %>% 
    mutate(x = factor(x, levels = xnames)) %>% 
    ggplot(mapping = aes(x = x)) +
    geom_hline(yintercept = 0, color = 'grey', linetype = 'dashed') +
    geom_point(mapping = aes(y = mu)) +
    geom_linerange(mapping = aes(ymin = mu - 2 * sd,
                                 ymax = mu + 2 * sd,
                                 group = x)) +
    labs(x = 'feature', y = 'coefficient value') +
    coord_flip() +
    theme_bw()
}
```

We create the posterior summary visualization figure for our best model, `Model 8`. 

```{r, 2B_8, eval = FALSE}
post_means_8 <- laplace_8$mode
post_sds_8 <- sqrt(diag(laplace_8$var_matrix))
feature_names_8 <- colnames(Xmat_8)

viz_post_coefs(post_means_8, post_sds_8, feature_names_8)
```

We can easily observe that coefficient plot for `Model 8` above and the corresponding plot in Part A) for `Model 8` have similar values and intervals for inputs.


### C) Linear models Predictions

We will make predictions with our 2 selected linear models, `Model 8` and `Model 6` in order to visualize the trends of the event probability with respect to the inputs. We will use non Bayesian models for the predictions.

As a reminder, 

`Model 8` is: `mod_3A_8 <- glm(outcome ~ splines::ns(R, 3) + splines::ns(G, 3) + splines::ns(B, 3) + splines::ns(Hue, 3) + Lightness + Saturation, data=ready_dfiiiA, family = "binomial")`

and 

`Model 6` is: `mod_3A_6 <- glm(outcome ~ (R + G + B + Hue)^2 + Lightness + Saturation, data = ready_dfiiiA, family = "binomial")`

Our primary input will be `R` and secondary input will be `Hue`. We decide the reference values to use for the remaining inputs.

The `plogis()` function in R is the logistic function, which is the inverse of the logit function. We will use it to calculate the confidence intervals. 

The following code chunk gives predictions for `Model 8` with Confidence Intervals, when Lightness = "saturated" and Saturation = "subdued".

```{r, 2C_1, eval = FALSE}
primary_seq <- seq(min(ready_dfiiiA$R), max(ready_dfiiiA$R), length.out = 100)

prediction_data <- expand.grid(
  R = primary_seq,
  G = mean(ready_dfiiiA$G),  
  B = mean(ready_dfiiiA$B), 
  Hue = seq(min(ready_dfiiiA$Hue), max(ready_dfiiiA$Hue), length.out = 6),
  Lightness = "saturated",
  Saturation = "subdued" 
)

preds <- predict(mod_3A_8, newdata = prediction_data, type = "link", se.fit = TRUE)

link_lwr <- preds$fit - 2 * preds$se.fit
link_upr <- preds$fit + 2 * preds$se.fit

prediction_data$prob <- plogis(preds$fit)
prediction_data$prob_lwr <- plogis(link_lwr)
prediction_data$prob_upr <- plogis(link_upr)

ggplot(prediction_data, aes(x = R, y = prob)) +
  geom_line() +
  geom_ribbon(aes(ymin = prob_lwr, ymax = prob_upr), fill = "blue", alpha = 0.2) +
  facet_wrap(~Hue, scales = "free_x") +
  labs(title = "Model 8: Predictive Probability Trend with Confidence Intervals",
       x = "R", y = "Predicted Probability") +
  theme_minimal()
```

The following code chunk gives predictions for `Model 6` with Confidence Intervals, when Lightness = "saturated" and Saturation = "subdued".

```{r, 2C_2, eval = FALSE}
primary_seq <- seq(min(ready_dfiiiA$R), max(ready_dfiiiA$R), length.out = 100)

prediction_data <- expand.grid(
  R = primary_seq,
  G = mean(ready_dfiiiA$G),  
  B = mean(ready_dfiiiA$B), 
  Hue = seq(min(ready_dfiiiA$Hue), max(ready_dfiiiA$Hue), length.out = 6),
  Lightness = "saturated",
  Saturation = "subdued" 
)

preds <- predict(mod_3A_6, newdata = prediction_data, type = "link", se.fit = TRUE)

link_lwr <- preds$fit - 2 * preds$se.fit
link_upr <- preds$fit + 2 * preds$se.fit

prediction_data$prob <- plogis(preds$fit)
prediction_data$prob_lwr <- plogis(link_lwr)
prediction_data$prob_upr <- plogis(link_upr)

ggplot(prediction_data, aes(x = R, y = prob)) +
  geom_line() +
  geom_ribbon(aes(ymin = prob_lwr, ymax = prob_upr), fill = "blue", alpha = 0.2) +
  facet_wrap(~Hue, scales = "free_x") +
  labs(title = "Model 6: Predictive Probability Trend with Confidence Intervals",
       x = "R", y = "Predicted Probability") +
  theme_minimal()
```

The following code chunk gives predictions for `Model 8` with Confidence Intervals, when Lightness = "dark" and Saturation = "muted".

```{r, 2C_3, eval = FALSE}
primary_seq <- seq(min(ready_dfiiiA$R), max(ready_dfiiiA$R), length.out = 100)

prediction_data <- expand.grid(
  R = primary_seq,
  G = mean(ready_dfiiiA$G),  
  B = mean(ready_dfiiiA$B), 
  Hue = seq(min(ready_dfiiiA$Hue), max(ready_dfiiiA$Hue), length.out = 6),
  Lightness = "dark",
  Saturation = "muted" 
)

preds <- predict(mod_3A_8, newdata = prediction_data, type = "link", se.fit = TRUE)

link_lwr <- preds$fit - 2 * preds$se.fit
link_upr <- preds$fit + 2 * preds$se.fit

prediction_data$prob <- plogis(preds$fit)
prediction_data$prob_lwr <- plogis(link_lwr)
prediction_data$prob_upr <- plogis(link_upr)

ggplot(prediction_data, aes(x = R, y = prob)) +
  geom_line() +
  geom_ribbon(aes(ymin = prob_lwr, ymax = prob_upr), fill = "blue", alpha = 0.2) +
  facet_wrap(~Hue, scales = "free_x") +
  labs(title = "Model 8: Predictive Probability Trend with Confidence Intervals",
       x = "R", y = "Predicted Probability") +
  theme_minimal()
```

The following code chunk gives predictions for `Model 6` with Confidence Intervals, when Lightness = "dark" and Saturation = "muted".

```{r, 2C_4, eval = FALSE}
primary_seq <- seq(min(ready_dfiiiA$R), max(ready_dfiiiA$R), length.out = 100)

prediction_data <- expand.grid(
  R = primary_seq,
  G = mean(ready_dfiiiA$G),  
  B = mean(ready_dfiiiA$B), 
  Hue = seq(min(ready_dfiiiA$Hue), max(ready_dfiiiA$Hue), length.out = 6),
  Lightness = "dark",
  Saturation = "muted" 
)

preds <- predict(mod_3A_6, newdata = prediction_data, type = "link", se.fit = TRUE)

link_lwr <- preds$fit - 2 * preds$se.fit
link_upr <- preds$fit + 2 * preds$se.fit

prediction_data$prob <- plogis(preds$fit)
prediction_data$prob_lwr <- plogis(link_lwr)
prediction_data$prob_upr <- plogis(link_upr)

ggplot(prediction_data, aes(x = R, y = prob)) +
  geom_line() +
  geom_ribbon(aes(ymin = prob_lwr, ymax = prob_upr), fill = "blue", alpha = 0.2) +
  facet_wrap(~Hue, scales = "free_x") +
  labs(title = "Model 6: Predictive Probability Trend with Confidence Intervals",
       x = "R", y = "Predicted Probability") +
  theme_minimal()
```

In the plots above we can observe that the mean predictive trends are consistent (except some fixed values of Hue input) between the 2 selected linear models, `Model 8` and `Model 6`. However, the confidence intervals are wider in the better model, `Model 8`.

### D) Train/tune with resampling

We will train, assess, tune, and compare more complex methods via resampling. We will use `caret` to handle the preprocessing, training, testing, and evaluation.

```{r, 2D}
library(caret)
```

We must specify a resampling scheme and a primary performance metric. Let’s use 5-fold cross-validation with 3-repeats. Our primary performance metric will be RMSE.

```{r, 2D_0}
my_ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3)

my_metric <- "Accuracy"
```

Below we train and tune 12 models:

1) All Categorical and Continuous Variables (Linear Additive):

```{r, 2D_1}
set.seed(2023)

fit_glm_1 <- train(outcome ~ R + G + B + Hue + Lightness + Saturation,
                  data = dfiiiD,
                  method = "glm",
                  metric = my_metric,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)

fit_glm_1
```

2) Add Categorical Inputs to All Main Effect and All Pairwise Interactions of Continuous Inputs (This is `Model 6` in Part A):

```{r, 2D_3}
set.seed(2023)

fit_glm_2 <- train(outcome ~ (R + G + B + Hue)^2 + Lightness + Saturation,
                  data = dfiiiD,
                  method = "glm",
                  metric = my_metric,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)

fit_glm_2
```

3) Add Categorical Inputs to 3 degree-of-freedom natural (DOF) spline from continuous variables (This is `Model 8` in Part A):

```{r, 2D_4}
set.seed(2023)

fit_glm_3 <- train(outcome ~ splines::ns(R, 3) + splines::ns(G, 3) + splines::ns(B, 3) + splines::ns(Hue, 3) + Lightness + Saturation,
                  data = dfiiiD,
                  method = "glm",
                  metric = my_metric,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)

fit_glm_3
```

4) Add Categorical Inputs to Interactions from 3 DOF spline from input R and All Pairwise Interactions of Continuous Inputs G, B, Hue (This is `Model 9` in Part A):

```{r, 2D_5}
set.seed(2023)

fit_glm_4 <- train(outcome ~ splines::ns(R, 3) * (G + B + Hue)^2 + Lightness + Saturation,
                  data = dfiiiD,
                  method = "glm",
                  metric = my_metric,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)

fit_glm_4
```

5) Elastic Net - Add Categorical Inputs to All Main Effect and All Pairwise Interactions of Continuous Inputs (This is `Model 6` in Part A):

```{r, 2D_6}
set.seed(2023)

fit_enet_1 <- train(outcome ~ (R + G + B + Hue)^2 + Lightness + Saturation,
                    data = dfiiiD,
                    method = "glmnet",
                    metric = my_metric,
                    preProcess = c("center", "scale"),
                    trControl = my_ctrl)

fit_enet_1
```

6) Elastic Net - Add Categorical Inputs to 3 degree-of-freedom natural (DOF) spline from continuous variables (This is `Model 8` in Part A):

```{r, 2D_7}
set.seed(2023)

fit_enet_2 <- train(outcome ~ splines::ns(R, 3) + splines::ns(G, 3) + splines::ns(B, 3) + splines::ns(Hue, 3) + Lightness + Saturation,
                    data = dfiiiD,
                    method = "glmnet",
                    metric = my_metric,
                    preProcess = c("center", "scale"),
                    trControl = my_ctrl)

fit_enet_2
```

7) Elastic Net - Add Categorical Inputs to Interactions from 3 DOF spline from input R and All Pairwise Interactions of Continuous Inputs G, B, Hue (This is `Model 9` in Part A):

```{r, 2D_8}
set.seed(2023)

fit_enet_3 <- train(outcome ~ splines::ns(R, 3) * (G + B + Hue)^2 + Lightness + Saturation,
                    data = dfiiiD,
                    method = "glmnet",
                    metric = my_metric,
                    preProcess = c("center", "scale"),
                    trControl = my_ctrl)

fit_enet_3
```

8) Neural network

```{r, 2D_9}
set.seed(2023)

fit_nnet <- train(outcome ~ .,
                    data = dfiiiD,
                    method = "nnet",
                    metric = my_metric,
                    preProcess = c("center", "scale"),
                    trControl = my_ctrl,
                    trace = FALSE)

fit_nnet
```

9) Random forest

```{r, 2D_10}
set.seed(2023)

fit_rf <- train(outcome ~ .,
                data = dfiiiD,
                method = "rf",
                metric = my_metric,
                trControl = my_ctrl,
                importance = TRUE)

fit_rf
```

10) Gradient boosted tree

```{r, 2D_11}
set.seed(2023)

fit_xgb <- train(outcome ~ ., 
                     data = dfiiiD, 
                     method = "xgbTree", 
                     metric = my_metric, 
                     trControl = my_ctrl, 
                     verbosity = 0,
                     nthread = 1)

plot(fit_xgb)
```

11) Support Vector Machines (SVM)

```{r, 2D_12}
set.seed(2023)

fit_svm <- train(outcome ~ .,
                 data = dfiiiD,
                 method = "svmRadial",
                 metric = my_metric,
                 preProcess = c("center", "scale"),
                 trControl = my_ctrl)

fit_svm
```

12) Partial least squares (PLS)

```{r, 2D_13}
pls_grid <- expand.grid(ncomp = 1:5)

set.seed(2023)

fit_pls <- train(outcome ~ .,
                 data = dfiiiD,
                 method = "pls",
                 metric = my_metric,
                 tuneGrid = pls_grid,
                 preProcess = c("center", "scale"),
                 trControl = my_ctrl)

fit_pls
```

Let's compare the models. We compile the resampling results together.

```{r, 2D_14}
my_results <- resamples(list(GLM_1 = fit_glm_1,
                             GLM_2 = fit_glm_2,
                             GLM_3 = fit_glm_3,
                             GLM_4 = fit_glm_4,
                             ENET_1 = fit_enet_1,
                             ENET_2 = fit_enet_2,
                             ENET_3 = fit_enet_3,
                             NNET = fit_nnet,
                             RF = fit_rf,
                             XGB = fit_xgb,
                             SVM = fit_svm,
                             PLS = fit_pls))
```

Compare models based on Accuracy.

```{r, 2D_15}
dotplot(my_results, metric = "Accuracy")
```

Based on the results above, the best 3 models are as follows:

1) `fit_xgb` - Gradient boosted tree

2) `fit_rf` - Random forest

3) `fit_glm_3` - Add Categorical Inputs to 3 degree-of-freedom natural (DOF) spline from continuous variables (This is `Model 8` in Part A)

We save the best classification model below:

```{r, 2D_17}
fit_xgb %>% readr::write_rds("best_classification_model.rds")
```


